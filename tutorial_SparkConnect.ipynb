{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbqsXqtc-8X9"
      },
      "source": [
        "# Use Spark Connect in Client Applications\n",
        "### Implemented by Simona Scala - A.Y. 2022/23"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZ3A8Xjq_Sxg"
      },
      "source": [
        "## Method 1: Manual Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57xFJkZC_vOg"
      },
      "source": [
        "To install Apache Spark on Google Colab manually, we need to follow a series of steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvSKrm7GD1kf"
      },
      "source": [
        "**Step 1**: The initial step is to download Java, as Spark relies on the Java Virtual Machine (JVM) to run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvmP1nStlZbE"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOTuzdaWD_El"
      },
      "source": [
        "**Step 2**: Download the latest version of Apache Spark\n",
        "- Go to Spark's [download page](https://spark.apache.org/downloads.html) and choose a Spark release version and a package type (the default is the latest version).\n",
        "- Click the link for downloading Spark, and you will be directed to a new web page.\n",
        "- Copy the first link on the web page, which is below the sentence \"*We suggest the following site for your download:*\"\n",
        "- Download Spark from the copied link.\n",
        "- Unzip the downloaded file to extract its contents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvapIkBE2psI"
      },
      "outputs": [],
      "source": [
        "!wget -q https://dlcdn.apache.org/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz   # download Spark\n",
        "!tar xf spark-3.4.1-bin-hadoop3.tgz   # unzip the file\n",
        "!ls -a    # list all files and directories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twj_yC9CE_a_"
      },
      "source": [
        "**Step 3**: Set up the environment variables for Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TqMPQ_A4lJp"
      },
      "outputs": [],
      "source": [
        "SPARK_VERSION = '3.4.1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcYBQ2G43pPd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.1-bin-hadoop3\"\n",
        "os.environ[\"SPARK_VERSION\"] = '3.4.1'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QP2Yxt4c42Zx"
      },
      "source": [
        "**Step 4**: Install and import the library for locating Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GbThXYUa43Th",
        "outputId": "67a60c86-49e5-4e4f-f67b-4ba9807c712d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/spark-3.4.1-bin-hadoop3'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install -q findspark\n",
        "import findspark\n",
        "findspark.init()    # initiate findspark\n",
        "findspark.find()    # check the location for Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mu4dly605MzS"
      },
      "source": [
        "**Step 5**: Test the installation by starting a \"traditional\" Spark session and check the session information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "cq1ZyzfR5F4S",
        "outputId": "45199f85-9575-417a-d9e1-1530e584e507"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://e556dd8aed2b:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7b1ad805aaa0>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "print(spark)   # check Spark Session Information\n",
        "print(type(spark))  # check the type of session"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISR2MHfl6d9k"
      },
      "source": [
        "**Step 6**: Now that the Spark server is running, we can connect to it remotely using Spark Connect. We do this by creating a remote Spark session on the client where our application runs. Before we can do that, we need to make sure to stop the existing regular Spark session because it cannot coexist with the remote Spark Connect session we are about to create."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrkHPivsDM2W"
      },
      "outputs": [],
      "source": [
        "SparkSession.builder.master(\"local[*]\").getOrCreate().stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eJvQobC6sft"
      },
      "source": [
        "At this point, we are ready launch the Spark server with the following `start-connect-server.sh` script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZfnxGPjE3ml",
        "outputId": "31c011bc-988d-4ed6-f00e-a960a32a0aa7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "starting org.apache.spark.sql.connect.service.SparkConnectServer, logging to /content/spark-3.4.1-bin-hadoop3/logs/spark--org.apache.spark.sql.connect.service.SparkConnectServer-1-e556dd8aed2b.out\n"
          ]
        }
      ],
      "source": [
        "!$SPARK_HOME/sbin/start-connect-server.sh --packages org.apache.spark:spark-connect_2.12:$SPARK_VERSION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCrnF-Z16lbH"
      },
      "source": [
        "The command we used above to launch the server configured Spark to run as `localhost:15002`.\n",
        "\n",
        "So now we can create a remote Spark session on the client and check the session information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzqJAS1KDT26"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.remote(\"sc://localhost:15002\").getOrCreate()\n",
        "print(spark)   # check Spark Session Information\n",
        "print(type(spark))  # check the type of session"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxVtvWpv_gAB"
      },
      "source": [
        "## Method 2: Automatic Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NJWZAKaMJ7s"
      },
      "source": [
        "The second method of installing PySpark on Google Colab is to use `pip install`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHFD6Qv46w7Z",
        "outputId": "90217892-4e43-4ce3-f545-2e747814edfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285397 sha256=a5725dd41af05dc2b78e0bf848eed6b0b832c48f9df28203aa7b0f0cfba979f7\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwKUHYJ1MfeS"
      },
      "source": [
        "To create a remote Spark session, we have to include the remote function with a reference to our Spark server when we create a Spark session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OcEkwCd64Rn",
        "outputId": "63d989b8-2992-4e6e-82c3-3cdc20f05ddc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "pyspark.sql.connect.session.SparkSession"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.remote(\"sc://localhost:15002\").getOrCreate()\n",
        "print(spark)   # check Spark Session Information\n",
        "print(type(spark))  # check the type of session"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Warning!"
      ],
      "metadata": {
        "id": "v8McVn8Bvk6L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Spark 3.4, Spark Connect supports most PySpark APIs, including [DataFrame](https://https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html), [Functions](https://https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html), and [Column](https://https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/column.html). However, some APIs such as [SparkContext](https://https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.html) and [RDD](https://https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.html) are not supported. You can check which APIs are currently supported in the [API reference](https://https://spark.apache.org/docs/latest/api/python/reference/index.html) documentation. Supported APIs are labeled “Supports Spark Connect” so you can check whether the APIs you are using are available before migrating existing code to Spark Connect."
      ],
      "metadata": {
        "id": "A7I04g_zwGGY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*N.B.: This notebook was last updated on July 2023*"
      ],
      "metadata": {
        "id": "rutGL8pOwoZN"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}